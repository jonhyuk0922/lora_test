{
    "train_batch_size": 4,
    "gradient_accumulation_steps": 2,
    "fp16": {
        "enabled": true,
        "initial_scale_power": 8
    },
    "zero_optimization": {
        "stage": 2
    },
    "sparse_attention": {
        "mode": "fixed",
        "block": 16,
        "different_layout_per_head": true,
        "num_local_blocks": 4,
        "num_global_blocks": 1,
        "attention": "bidirectional",
        "horizontal_global_attention": false,
        "num_different_global_patterns": 4
    },
    "inference": {
        "enabled": true,
        "dtype": "fp16"
    }
}